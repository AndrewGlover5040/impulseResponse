---
title: "New Algorithm"
author: "Andrew Glover"
date: "`r Sys.Date()`"
output: pdf_document
---
Setup
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
devtools::load_all()
```

## Libraries
```{r}
library(tibble)
library(ggplot2)
library(magrittr)
library(dplyr)
library(patchwork)
```


We are going to assume that we know $p_0$ exactly. That way it is either 
a data point in actual performance, with no cost, or we don't count it as 
an actual performance. 
# Algorithm Functions:
## Some helpers:
```{r}
lim_func <- function(k_1, k_2, tau_1, tau_2) {
  k_1/(1-exp(-1/tau_1))-k_2/(1-exp(-1/tau_2))
}

params_dist <- function(params_1, params_2) {
  # ignore params_1[[1]] since it is just the inital value
  abs(lim_func(params_1[[2]], params_1[[3]], params_1[[4]], params_1[[5]]) -
        lim_func(params_2[[2]], params_2[[3]], params_2[[4]], params_2[[5]])
      )
}

plot_perf <- function(observed_performance,
                      modeled_performance) {
  data <- tibble(
  "day" = c(0:length(observed_performance)),
  "pred" = modeled_performance,
  "obs" = c(NA,observed_performance)
  )
  
  plot <- ggplot(data, aes(x = day)) + 
  geom_point(aes(y = pred, color = "pred")) + 
  geom_point(aes(y = obs, color = "obs")) +
  labs(x = "Day",
       y = "Performance",
       title = "First go around of new algorithm") 
  plot
}
```


### first go-around of update params
```{r}
cost_perf <- function(params,
                      old_params,
                      training_load,
                      obs_perf,
                      lambda = 1
                      ) {
  sqrt(SSE(params, training_load, obs_perf)/length(obs_perf)) + # the SSE fits the suggested parameters
    # against the old data, might add a forgetting factor to localize the fitting
    ## added the square root because it seems that, if it is not there, 
    ## the SSE term will dominate the params_dist term for large( like 5) terms no 
    ## matter what the lamda term is
    lambda*params_dist(params, old_params) 
  #####################################################
  # might add some forgeting factor here can do later
  #####################################################
}

# params matrix in order k_1, k_2, tau_1, tau_2

update_params <- function(old_params, 
                          training_load,
                          sub_obs_perf,
                          bounds_type = list("test"),
                          lambda = 1 
                          ) {
  
  if(bounds_type == "test") {
    params_matrix <<- expand.grid(p_0 = 500,
                                  k_1 = c(1,2,3,4),
                                  k_2 = c(2,4,6,8),
                                  tau_1 = c(5:35),
                                  tau_2 = c(5:35))
  }
  params_matrix <- params_matrix[params_matrix$tau_1 <= params_matrix$tau_2,]

  cost_vec <- apply(params_matrix,
                  1, 
                  cost_perf,
                  old_params = old_params,
                  training_load = training_load,
                  obs_perf = sub_obs_perf,
                  lambda = lambda
                  )
  
  out_list <- list()
  out_list$opt_params <- params_matrix[which(cost_vec==min(cost_vec)), ]
  out_list$cost <- cost_vec[[which(cost_vec==min(cost_vec))]]
  out_list$mult_mins <- FALSE
  if (length(which(cost_vec==min(cost_vec)))>1) {
    out_list$mult_mins <- list(TRUE, which(cost_vec==min(cost_vec)))
  }
  return(out_list)
}
```

## plotting function 


## second go around
Trying to optimize the process of finding the optimized parameters
```{r}
min_params <- function(params_matrix,
                       training_load,
                       obs_perf,
                       old_params,
                       lambda = 1) {
  min_params <- old_params
  obs_indexes <- which(!is.na(obs_perf))
  n <- length(obs_indexes)
  min_cost <- sqrt(SSE(old_params, training_load, obs_perf)/n)
  for (i in 1:nrow(params_matrix)){
    params_i <- as.numeric(params_matrix[i, ])
    params_dist_i <- params_dist(params_i, old_params)
    SSE_i <- 0; cost_i <- 0
    p_0 <- params_i[[1]]
    k_1 <- params_i[[2]]; k_2 <- params_i[[3]]
    coef_1 <- exp(-1/params_i[[4]]); coef_2 <- exp(-1/params_i[[5]])
    T_1 <- 0; T_2 <- 0
    lower_index <- 1
    j <- 0
    for (new_index in obs_indexes) {
      for (t in lower_index:new_index) {
        T_1 <- coef_1*(T_1) + training_load[[t]]
        T_2 <- coef_2*(T_2) + training_load[[t]]
      }
      SSE_i <- SSE_i + (p_0 + k_1*T_1 - k_2*T_2 - obs_perf[[new_index]])^2
      j <- j + 1
      lower_index <- new_index + 1
      if(min_cost < sqrt(SSE_i/n) + lambda*params_dist_i) {
        break # goes to next set of parameters
        # for a lot of the sets of parameters, 
        # the condition should trigger on the first few data points
      }
      if (j == n) {
        min_cost <- sqrt(SSE_i/n) + lambda*params_dist_i
        min_params <- params_i
      
      }
      else {
        # do nothing; go to the next loop
      }
    }
  }
  return(list("cost" = min_cost, "opt_params" = min_params))
}
```

```{r, eval = FALSE, include = FALSE}
set.seed(2)
days_test <- 200
perf_sim_1 <- 
  perf_tv(
    p_0 = 500,
    k_1 = 1,
    tau_1 = 25,
    k_2 = 2,
    tau_2 = 10,
    days = days_test,
    training_stim = list("constant", 100)
  )$performance

noise_fn <- function(x,  sd){
  x+rnorm(1, mean = 0, sd = sd)
}

# perf_sim_1 = purrr::map_dbl(perf_sim_1, noise_fn, sd = .5)

# this is one off
samp_index <- c(sample(c(1:100), 50), sample(c(101:days_test), 10))
actual_perf <- rep(NA, days_test)
for (i in samp_index) {
  actual_perf[[i]] = perf_sim_1[[i]]
  
}

min_params(matrix(c(500, 1, 2, 25, 10, 500, 1,3,20,10), nrow = 2, ncol = 5, byrow = TRUE),
           c(rep(100, days_test)),
           actual_perf,
           old_params = c(500, 1, 2, 25, 10),
           lambda = 1
           )
matrix(c(500, 1, 2, 25, 10, 500, 1,3,20,10), nrow = 2, ncol = 5)
```


Applying it to the update function
```{r}
update_params_opt <- function(old_params, 
                          training_load,
                          sub_obs_perf,
                          bounds_type = list("test"),
                          lambda = 1 
                          ) {
  if(bounds_type == "test") {
    params_matrix <- expand.grid(p_0 = 500,
                                  k_1 = c(1,2,3,4),
                                  k_2 = c(2,4,6,8),
                                  tau_1 = c(5:35),
                                  tau_2 = c(5:35))
  }
  params_matrix <- params_matrix[params_matrix$tau_1 <= params_matrix$tau_2,]
  
  getting_params_ouput <- min_params(params_matrix,
                                  training_load,
                                  sub_obs_perf,
                                  old_params,
                                  lambda = lambda)
  
  out_list <- list()
  out_list$opt_params <- getting_params_ouput$opt_params
  out_list$cost <- getting_params_ouput$cost
  return(out_list)
}
```


```{r}
new_pred_perf <- function(init_params,
                          training_load,
                          obs_perf,
                          bounds_type = list("test"),
                          lambda = 1,
                          optimize = FALSE) {
  curr_params <- init_params 
  curr_cost <- 0
  ####
  # curr_params take form c(k_1, k_2, tau_1, tau_2), to work with the time-invariant
  # functions
  ###
  
  matrix_params <- matrix(0, nrow = length(training_load), ncol = 5)
  colnames(matrix_params) <- c("p_0", "k_1", "k_2", "tau_1", "tau_2")

  days <- length(training_load)
  perf_out <- c(rep(curr_params[[1]], days + 1))
  cost_vec <- c(rep(0, days + 1))
  T_1 <- 0
  T_2 <- 0
  
  for (i in 1:length(obs_perf )) {
    # update params if there is new information
    if (is.na(obs_perf[[i]]) == FALSE) {
      
      # so the old code is compatible with this, 
      if (optimize == FALSE) {
        params_output <- update_params(
          old_params = curr_params,
          training_load = training_load,
          sub_obs_perf = obs_perf[1:i],
          # the subset up to observation
          bounds_type = bounds_type,
          lambda = lambda
        )
        curr_params <- params_output$opt_params
        curr_cost <- params_output$cost
      }
      
      # this runs the new functions
      if (optimize == TRUE) {
        params_output <- update_params_opt(
          old_params = curr_params,
          training_load = training_load,
          sub_obs_perf = obs_perf[1:i],
          # the subset up to observation
          bounds_type = bounds_type,
          lambda = lambda
        )
        curr_params <- params_output$opt_params
        curr_cost <- params_output$cost
      }
    } else {
    } # pass
    T_1 <- exp(-1 / curr_params[[4]]) * T_1 + training_load[[i]]
    # training load index is i-1 since i starts at 2
    T_2 <- exp(-1 / curr_params[[5]]) * T_2 + training_load[[i]]
    perf_out[[i+1]] <-
      curr_params[[1]] + curr_params[[2]] * T_1 - curr_params[[3]] * T_2
    cost_vec[[i]] <- curr_cost 
    ##############
    # fix cost it is wrong, 
    #############
    matrix_params[i, ] <- curr_params
  }
  
  #plotting
  params_data <- tibble(
    "day" = 0:length(training_load),
    "k_1" = as.numeric(c(matrix_params[1, "k_1"], matrix_params[, "k_1"])),
    "k_2" = as.numeric(c(matrix_params[1, "k_2"], matrix_params[, "k_2"])),
    "tau_1" = as.numeric(c(matrix_params[1, "tau_1"], matrix_params[, "tau_1"])),
    "tau_2" = as.numeric(c(matrix_params[1, "tau_2"], matrix_params[, "tau_2"])),
    "p_0" = as.numeric(c(matrix_params[1, "p_0"], matrix_params[, "p_0"]))
  )
  k_plot <- ggplot(params_data, aes(x = day)) +
    geom_line(aes(y = k_1, color = "k_1")) +
    geom_line(aes(y = k_2, color = "k_2"))

  tau_plot <- ggplot(params_data, aes(x = day)) +
    geom_line(aes(y = tau_1, color = "tau_1")) +
    geom_line(aes(y = tau_2, color = "tau_2"))

  #outputs
  
  out_list <- list()
  out_list$performance <- perf_out
  out_list$cost_vec <- cost_vec
  out_list$perf_plot <- plot_perf(obs_perf, perf_out)
  out_list$params_plots <- k_plot + tau_plot
  return(out_list)
} 
```
If `lambda` is equal to $0$, then I think some weird underflow garbage happens
and the output is wack, but this seems to work for very very very small values 
of `lambda`. I should try to fix this later. 




```{r warning=FALSE}
set.seed(2)
days_test <- 200
perf_sim_1 <- c(500, perf_tv(p_0 = 500,
          k_1 = 1,
          tau_1 = 25, 
          k_2 = 2, 
          tau_2 = 10,
          days = days_test,
          training_stim = list("constant", 100))$performance
) 
noise_fn <- function(x,  sd){
  x+rnorm(1, mean = 0, sd = sd)
}

perf_sim_1 = purrr::map_dbl(perf_sim_1, noise_fn, sd = 100)

# this is one off
samp_index <- c(sample(c(1:100), 50), sample(c(101:days_test), 10))
actual_perf <- rep(NA, days_test)
for (i in samp_index) {
  actual_perf[[i]] = perf_sim_1[[i+1]]
  
}


# plot_check <- plot_perf(actual_perf,
#                         perf_sim_1) +
#   labs(title = "Check that sampling from the simulation works")
# plot_check

model_perf <- new_pred_perf(
  c(500, 1,2, 25, 10),
  c(rep(100, days_test)),
  actual_perf,
  lambda = 1,
  optimize = TRUE
)
model_perf$cost_vec
model_perf$perf_plot 
model_perf$params_plots

```



Inputs:
- Actual performance, vector of length $n$
- Training load, vector of length $n$
- Initial guess of parameters, vector of length $4$
- $p_0$, a number
- Ranges to search over for each variable, The length of these vectors 
multiply to $\ell$

Outputs, 

Algorithm:
(1) Accept inputs
(2) Define 
- `pred_perf`, a vector of length $n+1$, initalized with $p_0$
- `cur_param`, a vector of length $4$, used to keep track of the parameters used
on each step
- `cost`, a number to keep track of the cost function

For $i = 0,1\hdots, n$
If Actual Performance at $i$ is equal to $NA$,
  vvapply `curr_param` to predicted performance recursive equation to get predicted 
  performance
  set `pred_perf[[i]]` to be this predicted performance





## Testing New Algorthm
```{r, warning = FALSE}
days_test <- 200
test_tr <- c(rep(100, days_test))
test_params <- c(500, 1, 2, 25, 10)
true_model <- invariant_perf(c(500, 1, 2, 25, 10), training_load = test_tr)

obs_perf_test <- c(rep(NA, 19), 100, rep(NA, 19), 475, 
              rep(NA,19), 730, rep(NA, 19), 860, 
              rep(NA, 19), 900, rep(NA, days_test - 100))

test_pred_perf <- new_pred_perf(
  init_params = test_params,
  training_load = test_tr ,
  obs_perf = obs_perf_test,
  lambda = 2.5 # found this value by trial and error
)

####################################
# add p_0 to invariand_perf, b/c (star)
####################################


data_test <- tibble(
  "day" = c(0:length(test_tr)),
  "pred_perf" = test_pred_perf,
  "obs_perf" = c(NA, obs_perf_test),
  "true_perf" = c(500, true_model) # (star)
)


test_plot <- ggplot(data = data_test, aes(x = day)) + 
  geom_line(aes(y = pred_perf, color = "pred")) + 
  geom_point(aes(y = obs_perf, color = "obs")) +
  geom_point(aes(y = true_perf, color = "act")) +
  labs(x = "Day",
       y = "Performance",
       title = "First go around of new algorithm") 

test_plot
```
This looks promising. The new curve gets close to but is not the exact curve, 
suggesting that it has the capability to not over or under fit. 


## More complicated simulation 

```{r}
set.seed(1)
days_test <- 500
perf_sim_1 <- c(500, perf_tv(p_0 = 500,
          k_1 = c(1,2,3),
          tau_1 = c(20, 25, 30), 
          k_2 = c(2, 4, 6), 
          tau_2 = c(5,10, 15),
          change_days = c(100, 200), 
          days = days_test,
          training_stim = list("constant", 100))$performance
)
samp_index <- c(sample(c(1:100), 10), sample(c(101:days_test), 20))
actual_perf <- rep(NA, days_test)
for (i in samp_index) {
  actual_perf[[i]] = perf_sim_1[[i]]
}

plot_check <- plot_perf(actual_perf,
                        perf_sim_1) +
  labs(title = "Check that sampling from the simulation works")
plot_check

model_perf <- new_pred_perf(
  c(500, 1,2, 25, 10),
  c(rep(100, days_test)),
  actual_perf,
  lambda = 2
)


length(model_perf)
plot_perf(actual_perf,
          model_perf)
```




## applying to real data
```{r}
real_data <- data_2
day_vec_real <- real_data[[1]]
training_load_real <- real_data[[2]]
obs_perf_real <- real_data[[3]]
init_params_real <- c(262, .1, .1, 11, 11)
obs_new_alg <- new_pred_perf(
  init_params_real,
  training_load_real,
  obs_perf_real,
  lambda = .000000000005
)

data_real <- tibble(
  "day" = c(0, day_vec_real),
  "pred" = obs_new_alg,
  "obs" = c(NA, obs_perf_real)
)

real_plot <- ggplot(data = data_real, aes(x = day)) + 
  geom_point(aes(y = pred, color = "pred")) + 
  geom_point(aes(y = obs, color = "obs")) +
  labs(x = "Day",
       y = "Performance",
       title = "First go around of new algorithm") 

real_plot


```
This code gave the same output for many lambda greater than the one used, 
suggesting 






