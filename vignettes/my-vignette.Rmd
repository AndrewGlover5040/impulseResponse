---
title: "my-vignette"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{my-vignette}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(devtools)
library(ggplot2)
library(readr)


load_all() ###The original way wasn't working. 

# Using winther_data and clarke_data 
# clarke data = data_1
# winther data = data_2
# The data is located in impulseResponse\data
```

# What This Package is About 
In this Vignette, we will give a brief literature review of the Banister Impulse-Response model, a description of the model, and then a demonstration of the functions in this package. There have been two versions of the impulse-response model discussed in the literature, the original in which the parameters are time-invariant, and a version that was developed later, in which the parameters are allowed to vary over time. The main functions are `invariant_perf` and `RLS_predicted_performance`, which are designed to model the time-invariant and time-variant versions, respectively. These estimate the model parameters given training data.  

## Literature Review
The Impulse-Response model was introduced in 1975 by Banister et al. (citation) to model athletic performance from training load. It has been used over the years to model athletic performance when the output can be easily quantified, such as swimming, cycling, power lifting, running, and the hammer throw. (CITE PAPERS WITH THESE APPLICATIONS). (START NEW PARAGRAPH)To our knowledge, the time-invariant model was first introduced in 1997 by Busso (citation). Vermeire et al. provide a good explanation of the limitations of this model (citation). 

Previously, there has been a lack of publicly available computational resources for estimating and applying the model. However, Clarke and Skiba give a great introduction and overview of this model and apply it using Excel (citation). Also, this [blog post](https://wintherperformance.netlify.app/post/banister-model/)
by Andreas K. Winther applies the model in R, and was helpful in creating this package. 

# Impulse-Response model
The Banister Impulse-Response model models athletic performance as a function of training load. Roughly, it says that performance can be modeled as the sum of the ability of the athlete before training begins, the positive effects of training (PTE), and the negative effects of training (NTE). Formally, it is defined as,
$$
P(n)=p_0+\overbrace{k_1\sum_{i=0}^{n-1}e^{-(n-i)/\tau_1}w(i)}^{\text{PTE}}-\overbrace{k_2\sum_{i=0}^{n-1}e^{-(n-i)/\tau_2}w(i)}^{\text{NTE}}
$$
where p_0 is the initial performance, $w(i)$ is the training load on day $i$.  $k_1,~k_2,~\tau_1,$ and $\tau_2$ are real constants. 


To gain some intuition behind the model, we focus on just PTE and ignore $k_1$. Then, we have 
$$
PTE(n) \propto e^{-1/\tau_1}w(n-1)+e^{-2/\tau_1}w(n-2)+\dots+e^{-n/\tau_1}w(0)
$$
If $\tau_1\geq 1$ (this is not restrictive; it makes the model work for any application) then the exponential term in front of the workload decreases as the days decrease; the model gives high weight to recent training days, and low weights to less recent training days. The $\tau$ term controls how quickly we ``forget'' about the past days. The $k$ term in front of the sum then controls how well the body can convert the benefit from the training load to performance. These parameters vary from person to person. 

## The Time-invariant Model: The `invariant_perf` Function

We described the time-invariant model above, as it assumes the parameters $k_1,~k_2,~\tau_1,$ and $\tau_2$ are fixed across time. 

The `invariant_perf` function provides the model predicted athletic performances given vectors of training loads and the person-specific parameters listed above. This will be illustrated in a simple example. If we apply this function to a constant training load (`training_load_ex`) for $100$ days, and some nicely (WHAT DOES NICELY MEAN?) chosen parameters (`params_ex`) we get:
<!-- make sure that y-axis in graph looks correct -->


```{r}
params_ex <- c(250, #p_0
               20,  #k_1
               10,  #k_2
               15,  #tau_1
               21   #tau_2
               )
training_load_ex <- rep(10, 100)
inv_perf_ex <- invariant_perf(params_ex, training_load_ex)
data_ex <- tibble::tibble(
  "day" = c(1:100),
  "perf" = inv_perf_ex
)
plot_ex <- ggplot(data_ex, aes(x=day, y=perf))+
  geom_line(color="blue")
plot_ex

```
This model, with the given parameters captures a few phenomenon training with a constant training load. These are the initial gain from the workout, the plateau, and the taper. 
<!-- citation; prehaps Winther or Busso-->

We can closely approximate the optimal parameters (with respect to SSE) (YOU SHOULD EXPAND THIS WITH A COUPLE OF SENTENCES, I.E. THE SSE IS THE OBJECTIVE FUNCTION, ETC.) using the `optim_par` function, which amounts to using R's built in `optim` function. If we model this against real data. 

```{r, warning=FALSE}
params_guess <- c(250,11,11,30,30)

optim_params_1 <- optim_par(params_guess, 
                                  data_1[[2]], #training load of one athlete
                                  data_1[[3]], #actual performance of one athlete
)
perf <- invariant_perf(optim_params_1, data_1[[2]])
tib_1 <- tibble::tibble(
  "day" = data_1[[1]],
  "predicted_performance" = perf, 
  "actual_performance" = data_1[[3]] 
)

plot_1 <- ggplot(tib_1, aes(x=day, y=predicted_performance))+
  geom_line(color="blue")+
  geom_point(aes(y=actual_performance))
plot_1



```

This model does have limitations. In some situations it is reasonable that the parameters might vary. Looking at the Winther data:

```{r, warning=FALSE} 
params_guess_2 <- c(250,11,11,30,30)
optim_params_2 <- optim_par(params_guess_2, 
                                  data_2[[2]], #training load of one athlete
                                  data_2[[3]], #actual performance of one athlete
)
print(optim_params_2)
perf <- invariant_perf(optim_params_2, data_2[[2]])
tib_2 <- tibble::tibble(
  "day" = data_2[[1]],
  "predicted_performance" = perf, 
  "actual_performance" = data_2[[3]] 
)

plot_2 <- ggplot(tib_2, aes(x=day, y=predicted_performance))+
  geom_line(color="blue")+
  geom_point(aes(y=actual_performance))
plot_2
# Getting the actual performance
```


The time invariant model tracks the performance at the beginning of the graph, but then misses some of the points later. Overall, it gets pretty close to the actual performances. This suggests that under some circumstances, the parameters of the athletes change over time. Considering training effects such as over training, and insufficient recovery, this makes intuitive sense. The main point of this package is to provide a model that can do so. 

# Time-varying Model: The `RLS_predicted_performance` Function

(NEED TO MENTION WHAT RLS MEANS, CITE THE BOOK THE ALGORITHM IS BASED ON). The `RLS_predicted_performance` function is able to update our guess of what the parameter values are with each new data point using Recursive Least Squares. For instance, it performs better on the Winther data set than the Time-invariant model. 

```{r, warning=FALSE}

perf_RLS_2 <- RLS_predicted_performance(
    training_load = data_2[[2]],
    performance = data_2[[3]], 
    p_0 = 250,
    alpha = .9, #note this input
    bounds_T_1 = c(1,50),
    by_T_1 = 1,
    bounds_T_2 = c(1,50),
    by_T_2 = 1,
    good_output = TRUE
)


RLS_tib_2 <- tibble::tibble(
  "day" = data_2[[1]],
  "predicted_performance" =  perf_RLS_2[[1]], #this function returns a list of things, the first one is the predicted performance
  "actual_performance" = data_2[[3]] 
)


plot_RLS_2 <- ggplot(RLS_tib_2, aes(x=day, y=predicted_performance))+
  geom_line(color="blue")+
  geom_point(aes(y=actual_performance))
plot_RLS_2

```

There is a bit of a "burn-in" period, but the Time-Invariant model is able to track the performance better. 

The cost of this flexibility is more complexity in the model. The value of `alpha` determines how much the model ''forgets'' past data values. Alpha takes values from one (inclusive) to zero (not inclusive). An `alpha` value of $1$ means that there is no forgetting going on in the model, and a low `alpha` value means that the model quickly for gets past data values. Applying the Time-varying model with an alpha value of $1$ to the Winther Dataset:

<!-- Change winther to dataset 2, or something boring, since it came from Busso -->

```{r, warning=FALSE}

perf_RLS_2_new <- RLS_predicted_performance(
    training_load = data_2[[2]],
    performance = data_2[[3]], 
    p_0 = 250,
    alpha = 1, ### Not .9!!!
    bounds_T_1 = c(1,50),
    by_T_1 = 1,
    bounds_T_2 = c(1,50),
    by_T_2 = 1,
    good_output = TRUE
)


RLS_tib <- tibble::tibble(
  "day" = data_2[[1]],
  "predicted_performance" =  perf_RLS_2_new[[1]], #this function returns a list of things, the first one is the predicted performance
  "actual_performance" = data_2[[3]] 
)


plot_new_2 <- ggplot(RLS_tib, aes(x=day, y=predicted_performance))+
  geom_line(color="blue")+
  geom_point(aes(y=actual_performance))
plot_new_2
```
The model is pretty smooth, still with a burn-in period, but it misses some of 
the data points at the end of the data set. 

Conversely, if we use an `alpha` value of $.01$

```{r, warning=FALSE}

perf_bad_alpha <- RLS_predicted_performance(
    training_load = data_2[[2]],
    performance = data_2[[3]], 
    p_0 = 250,
    alpha = .01, ### Not .9!!!
    bounds_T_1 = c(1,50),
    by_T_1 = 1,
    bounds_T_2 = c(1,50),
    by_T_2 = 1,
    good_output = TRUE
)


tib_RLS <- tibble::tibble(
  "day" = data_2[[1]],
  "predicted_performance" =  perf_bad_alpha[[1]], #this function returns a list of things, the first one is the predicted performance
  "actual_performance" = data_2[[3]] 
)


plot_data_2 <- ggplot(tib_RLS, aes(x=day, y=predicted_performance))+
  geom_line(color="blue")+
  geom_point(aes(y=actual_performance))
plot_data_2
```
The model interpolates the points correctly -- there is very little bias --
but there is a lot of variance. This is a classic example of the bias-variance tradeoff.







