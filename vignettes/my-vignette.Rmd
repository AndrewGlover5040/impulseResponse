---
title: "my-vignette"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{my-vignette}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(devtools)
library(ggplot2)
library(readr)


load_all() ###The original way wasn't working. 

# Using winther_data and clarke_data 
# clarke data = data_1
# winther data = data_2
# The data is located in impulseResponse\data
```

# What This Package is About 
In this Vignette, we will give a brief literature review of the Banister Impulse-Response model, a description of the model, and then a demonstration of the functions in this package. 

## Literature Review
The Impulse-Response model was introduced in 1975 by Banister et al. (citation) to model athletic performance from training load. It has been used over the years to model athletic performance when the output can be easily quantified, such as swimming, cycling, power lifting, running, and the hammer throw. To our knowledge, the time-invariant model was first introduced in 1997 by Busso (citation). Vermeire et al. provide a good explantion of the limitations of this model (citation). There are not many resources applying the Impulse-Response model. However, Clarke and Skiba give a great introduction and overview of this model and apply it using Excel (citation). This [blog post](https://wintherperformance.netlify.app/post/banister-model/)
by Andreas K. Winther applies the model in R, and was helpful in creating this package. 

# Impulse-Response model
The Banister Impulse-Response model models athletic performance as a function of training load. Roughly, it says that performance can be modeled as the sum of the ability of the athlete before training begins, the positive effects of training (PTE), and the negative effects of training (NTE). Formally, it is defined as,
$$
P(n)=p_0+\overbrace{k_1\sum_{i=0}^{n-1}e^{-(n-i)/\tau_1}w(i)}^{\text{PTE}}-\overbrace{k_2\sum_{i=0}^{n-1}e^{-(n-i)/\tau_2}w(i)}^{\text{NTE}}
$$
where p_0 is the initial performance, $w(i)$ is the training load on day $i$.  $k_1,~k_2,~\tau_1,$ and $\tau_2$ are real constants. 


Allow me to give some intuition behind the model. If we focus in on just PTE, and ignore $k_1$, we have 
$$
PTE(n) \propto e^{-1/\tau_1}w(n-1)+e^{-2/\tau_1}w(n-2)+\dots+e^{-n/\tau_1}w(0)
$$
If $\tau_1\geq 1$ (this is not restrictive; it makes the model work for any application) then the exponential term in front of the workload decreases as the days decrease; the model gives high weight to recent training days, and low weights to less recent training days. The $\tau$ term controls how quickly we ``forget'' about the past days. The $k$ term in front of the sum then controls how well the body can convert the benefit from the training load to performance. These parameters vary from person to person. 

## The Time-invariant Model: The `invariant_perf` Function

The time-invariant Impulse Response model assumes that the parameters $k_1,~k_2,~\tau_1,$ and $\tau_2$ are fixed across time. 

Lets use `invariant_perf` function on a simple example. If we apply this function to a constant training load (`training_load_ex`) for $100$ days, and some nicely chosen parameters (`params_ex`) we get:
<!-- make sure that y-axis in graph looks correct -->


```{r}
params_ex <- c(250, 20, 10, 15, 21)
training_load_ex <- rep(10, 100)
inv_perf_ex <- invariant_perf(params_ex, training_load_ex)
data_ex <- tibble::tibble(
  "day" = c(1:100),
  "perf" = inv_perf_ex
)
plot_ex <- ggplot(data_ex, aes(x=day, y=perf))+
  geom_line(color="blue")
plot_ex

```
This model, with the given parameters captures a few phenomenon training with a constant training load. These are the initial gain from the workout, the plateau, and the taper. 
<!-- citation; prehaps Winther or Busso-->

We can closely approximate the optimal parameters (with respect to SSE) using the `optim_par` function, which amounts to using R's built in `optim` function. If we model this against real data. 

```{r, warning=FALSE}
params_guess <- c(250,11,11,30,30)

optim_params_1 <- optim_par(params_guess, 
                                  data_1[[2]], #training load of one athlete
                                  data_1[[3]], #actual performance of one athlete
)
perf <- invariant_perf(optim_params_1, data_1[[2]])
tib_1 <- tibble::tibble(
  "day" = data_1[[1]],
  "predicted_performance" = perf, 
  "actual_performance" = data_1[[3]] 
)

plot_1 <- ggplot(tib_1, aes(x=day, y=predicted_performance))+
  geom_line(color="blue")+
  geom_point(aes(y=actual_performance))
plot_1



```

This model does have limitations. In some situations it is reasonable that the paramters might vary. Looking at the Winther data:

```{r, warning=FALSE} 
params_guess_2 <- c(250,11,11,30,30)
optim_params_2 <- optim_par(params_guess_2, 
                                  data_2[[2]], #training load of one athlete
                                  data_2[[3]], #actual performance of one athlete
)
print(optim_params_2)
perf <- invariant_perf(optim_params_2, data_2[[2]])
tib_2 <- tibble::tibble(
  "day" = data_2[[1]],
  "predicted_performance" = perf, 
  "actual_performance" = data_2[[3]] 
)

plot_2 <- ggplot(tib_2, aes(x=day, y=predicted_performance))+
  geom_line(color="blue")+
  geom_point(aes(y=actual_performance))
plot_2
# Getting the actual performance
```


The time invariant model tracks the performance at the beginning of the graph, but then misses some of the points later. Overall, it gets pretty close to the actual performances. This suggests that under some circumstances, the parameters of the athletes change over time. Considering training effects such as over training, and insufficient recovery, this makes intuitive sense. The main point of this package is to provide a model that can do so. 

# Time-varying Model: The `RLS_predicted_performance` Function

The `RLS_predicted_performance` function is able to update our guess of what the parameter values are with each new data point using Recursive Least Squares. For instance, it performs better on the Winther data set than the Time-invariant model. 

```{r, warning=FALSE}

perf_RLS_2 <- RLS_predicted_performance(
    training_load = data_2[[2]],
    performance = data_2[[3]], 
    p_0 = 250,
    alpha = .9, #note this input
    bounds_T_1 = c(1,50),
    by_T_1 = 1,
    bounds_T_2 = c(1,50),
    by_T_2 = 1,
    good_output = TRUE
)


RLS_tib_2 <- tibble::tibble(
  "day" = data_2[[1]],
  "predicted_performance" =  perf_RLS_2[[1]], #this function returns a list of things, the first one is the predicted performance
  "actual_performance" = data_2[[3]] 
)


plot_RLS_2 <- ggplot(RLS_tib_2, aes(x=day, y=predicted_performance))+
  geom_line(color="blue")+
  geom_point(aes(y=actual_performance))
plot_RLS_2

```

There is a bit of a "burn-in" period, but the Time-Invariant model is able to track the performance better. 

The cost of this flexibility is more complexity in the model. The value of `alpha` determines how much the model ''forgets'' past data values. Alpha takes values from one (inclusive) to zero (not inclusive). An `alpha` value of $1$ means that there is no forgetting going on in the model, and a low `alpha` value means that the model quickly for gets past data values. Applying the Time-varying model with an alpha value of $1$ to the Winther Dataset:

<!-- Change winther to dataset 2, or something boring, since it came from Busso -->

```{r, warning=FALSE}

perf_RLS_2_new <- RLS_predicted_performance(
    training_load = data_2[[2]],
    performance = data_2[[3]], 
    p_0 = 250,
    alpha = 1, ### Not .9!!!
    bounds_T_1 = c(1,50),
    by_T_1 = 1,
    bounds_T_2 = c(1,50),
    by_T_2 = 1,
    good_output = TRUE
)


RLS_tib <- tibble::tibble(
  "day" = data_2[[1]],
  "predicted_performance" =  perf_RLS_2_new[[1]], #this function returns a list of things, the first one is the predicted performance
  "actual_performance" = data_2[[3]] 
)


plot_new_2 <- ggplot(RLS_tib, aes(x=day, y=predicted_performance))+
  geom_line(color="blue")+
  geom_point(aes(y=actual_performance))
plot_new_2
```
The model is pretty smooth, still with a burn-in period, but it misses some of 
the data points at the end of the data set. 

Conversely, if we use an `alpha` value of $.01$

```{r, warning=FALSE}

perf_bad_alpha <- RLS_predicted_performance(
    training_load = data_2[[2]],
    performance = data_2[[3]], 
    p_0 = 250,
    alpha = .01, ### Not .9!!!
    bounds_T_1 = c(1,50),
    by_T_1 = 1,
    bounds_T_2 = c(1,50),
    by_T_2 = 1,
    good_output = TRUE
)


tib_RLS <- tibble::tibble(
  "day" = data_2[[1]],
  "predicted_performance" =  perf_bad_alpha[[1]], #this function returns a list of things, the first one is the predicted performance
  "actual_performance" = data_2[[3]] 
)


plot_data_2 <- ggplot(tib_RLS, aes(x=day, y=predicted_performance))+
  geom_line(color="blue")+
  geom_point(aes(y=actual_performance))
plot_data_2
```
The model interpolates the points correctly -- there is very little bias --
but there is a lot of variance. This is a classic example of the bias-variance tradeoff.







