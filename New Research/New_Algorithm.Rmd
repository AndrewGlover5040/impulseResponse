---
title: "New_Algorithm"
author: "Andrew Glover"
date: "`r Sys.Date()`"
output: 
  pdf_document:
    extra_dependencies: ["algorithm2e", "verbatim"]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(purl = FALSE)
```

## Setup
### Loading Package Functions
```{r loading package functions, message = FALSE}
devtools::load_all() 
```

### Libraries Used
```{r, message = FALSE, purl = TRUE}
library(tibble)
library(ggplot2)
library(magrittr)
library(dplyr) 
library(patchwork)
# loads the functions from the package
```

```{r purling, eval=FALSE, include=FALSE}
#### run this to purl the r code to the /R file, this seems to not be able to run with the knit
knitr::purl(input = "New_Algorithm.Rmd",
            output = stringr::str_c(rprojroot::find_rstudio_root_file(),"/R/New_Algorithm.R") #puts file in /R)
            ) 

``` 

We are going to assume that we know $p_0$ exactly. That way it is either 
a data point in actual performance, with no cost, or we don't count it as 
an actual performance. 

# Algorithm Functions:

## Helpers
```{r helpers, purl = TRUE}
# function inside of parameter distance
lim_func <- function(k_1, k_2, tau_1, tau_2) {
  k_1*exp(-1/tau_1)/(1-exp(-1/tau_1))-k_2*exp(-1/tau_2)/(1-exp(-1/tau_2))
}

# computes "parameter distance"
params_dist <- function(params_1, params_2, lambda) {
  # ignore params_1[[1]] since it is just the inital value
  lambda *abs(lim_func(params_1[[2]], params_1[[3]], params_1[[4]], params_1[[5]]) -
        lim_func(params_2[[2]], params_2[[3]], params_2[[4]], params_2[[5]])
      )
}

# plotting function
plot_perf <- function(observed_performance,
                      modeled_performance) {
  data <- tibble(
  "day" = c(0:length(observed_performance)),
  "pred" = modeled_performance,
  "obs" = c(NA,observed_performance)
  )
  
  plot <- ggplot(data, aes(x = day)) + 
  geom_point(aes(y = pred, color = "pred")) + 
  geom_point(aes(y = obs, color = "obs")) +
  labs(x = "Day",
       y = "Performance",
       title = "") 
  plot
}
```


```{r how to update parameters}
update_cost <- function(params_i, prev_params, # these two are the only things that change each iteration
                        obs_indexes, training_load, obs_perf,
                        error_func, cost_error_func, param_cost_func, 
                        alpha, window, 
                        min_cost_vec, min_params_mat
                     ) {
  # initializations
  params_cost_i <- param_cost_func(params_i, prev_params)
  cuml_error <- 0
  p_0 <- params_i[[1]]
  k_1 <- params_i[[2]]; k_2 <- params_i[[3]]
  coef_1 <- exp(-1/params_i[[4]]); coef_2 <- exp(-1/params_i[[5]])
  T_1 <- 0; T_2 <- 0
  lower_index <- 1
  j <- 0
  if (window > 0) { # 0 is the case where no window is specified
    error_tmp <- c(rep(0, window))
  }
  
  for (new_index in obs_indexes) {
    for (t in lower_index:new_index) {
      T_1 <- coef_1*(T_1 + k_1*training_load[[t]])
      T_2 <- coef_2*(T_2 + k_2*training_load[[t]])
    }
    error_i <- error_func(p_0 + T_1 - T_2 - obs_perf[[new_index]])
    cuml_error <- alpha*cuml_error + error_i 
    if(window > 0) {
      cuml_error <- cuml_error - alpha^(window)*error_tmp[[j%%window+1]]
      error_tmp[[j%%window+1]] <- error_i 
    }
    j <- j + 1
    lower_index <- new_index + 1
    cost_ij <- cost_error_func(cuml_error, j) + params_cost_i
    if (cost_ij < min_cost_vec[[j]]) {
      min_cost_vec[[j]] <- cost_ij
      min_params_mat[j, ] <- params_i
    }
  }
  return(list("params" = min_params_mat, "cost" = min_cost_vec))
}

min_params <- function(params_matrix, prev_params, # these two could change
                      training_load, obs_perf, # these won't; they are data
                      alpha, window, # these won't; they are specified by the user
                      error_func, cost_error_func, param_cost_func # these won't; they are specified by the user
                      ) {
  obs_indexes <- which(!is.na(obs_perf))
  n <- length(obs_indexes)
  min_cost_vec <- c(rep(Inf, n))
  min_params_mat <- matrix(0, nrow = n, ncol = 5) 
  for (i in 1:nrow(params_matrix)) {
    params_i <- as.numeric(params_matrix[i, ])
    res <- update_cost(params_i, prev_params, obs_indexes, training_load, obs_perf,
                       error_func, cost_error_func, param_cost_func,
                       alpha, window,
                       min_cost_vec, min_params_mat) 
    min_cost_vec <- res$cost
    min_params_mat <- res$params
  }
  return(list("opt_params" = min_params_mat, "cost" = min_cost_vec))
}

# this function creates (or calls, we will see) the parameter matrix and applies 
# it to the previous function. makes things tider in the algorithm funciton
searh_params_mat <- function(bounds_type = list("test")) {
  if(bounds_type == "test") {
    params_matrix <- expand.grid(p_0 = 500,
                                  k_1 = c(1,2,3,4),
                                  k_2 = c(2,4,6,8),
                                  tau_1 = c(5:35),
                                  tau_2 = c(5:35))
  }
  params_matrix <- params_matrix[params_matrix$tau_1 >= params_matrix$tau_2,]
  return(params_matrix)
}
```


```{r , purl = TRUE}
new_pred_perf <- function(init_params,
                          training_load,
                          obs_perf,
                          error_settings,
                          params_settings,
                          bounds_type = list("test")) {
  
  curr_params <- init_params 
  ####
  # curr_params take form c(p_0, k_1, k_2, tau_1, tau_2)
  ###
  matrix_params <- matrix(0, nrow = length(training_load), ncol = 5)
  colnames(matrix_params) <- c("p_0", "k_1", "k_2", "tau_1", "tau_2")
  days <- length(training_load)
  perf_out <- c(rep(curr_params[[1]], days + 1)) 
  
  type_error <- error_settings$type
  
  if (type_error %in% c("AE", "MAE", "AEOS")) {
    error_func <- abs
  } else if (type_error %in% c("SSE", "MSE", "RMSE")) {
    error_func <- function(x) x^2
  } else if (type_error %in% c("SPE", "MPE", "RMPE")) {
    p <- error_settings$p
    error_func <- function(x) x^p
  }
  
  else stop("change error settings")
  
  cost_error_func <- switch(type_error,
    "AE" = function(error, j) {error},
    "MAE" = function(error, j) {error/j},
    "AEOS" = function(error, j ) {error/sqrt(j)},
    "SSE" = function(error, j) {error},
    "MSE" = function(error, j) {error/j},
    "RMSE" = function(error, j) {sqrt(error/j)},
    "SPE" = function(error, j) {error},
    "MPE" = function(error, j) {error/j},
    "RMPE" = function(error, j) {(error/j)^{1/p}},
  )
  
  if (params_settings$type == "lim_cost") {
    param_cost_func <- function(x,y) {params_dist(x, y, params_settings$lambda)}
  }
  
  #### checking inputs ####
  alpha <- error_settings$alpha
  if (is.null(alpha) == TRUE) {
    alpha = 1
  } else if (!is.numeric(alpha)){
    stop("bad value for alpha, must be a real number")
  } else if (alpha > 1 || alpha < 0) {
    warning("Alpha is intended to be a real number between 0 and 1") 
  } else {
    #pass 
  }

  window <- error_settings$window
  if(is.null(window)==TRUE || is.infinite(window)) {
    window = 0
  } else if (!is.numeric(window)|| window - floor(window) > 0 || window < 0) {
    stop("Bad Value of alpha; must be a positive integer")
  } else {
    #pass
  }
  
  res_1 <- min_params(searh_params_mat(bounds_type = bounds_type),
                      init_params, training_load, obs_perf,
                      alpha, window,  
                      error_func, cost_error_func, param_cost_func)
  opt_params_mat <- res_1$opt_params
  cost_vec <- res_1$cost
  
  # doing the algorithm
  
  T_1 <- 0
  T_2 <- 0
  j = 1
  for (i in 1:length(obs_perf)) {
    # update params if there is new information
    if (is.na(obs_perf[[i]]) == FALSE) {
      curr_params <- as.numeric(opt_params_mat[j, ])
      j <- j + 1
    } else {
    } # pass
    T_1 <- exp(-1 / curr_params[[4]]) * (T_1 + curr_params[[2]] * training_load[[i]])
    # training load index is  since i starts at 2
    T_2 <- exp(-1 / curr_params[[5]]) * (T_2 + curr_params[[3]] * training_load[[i]])
    perf_out[[i+1]] <- curr_params[[1]] +  T_1 - T_2
    matrix_params[i, ] <- as.numeric(curr_params)
  }
  
  #plotting
  params_data <- tibble(
    "day" = 0:length(training_load),
    "k_1" = as.numeric(c(matrix_params[1, "k_1"], matrix_params[, "k_1"])),
    "k_2" = as.numeric(c(matrix_params[1, "k_2"], matrix_params[, "k_2"])),
    "tau_1" = as.numeric(c(matrix_params[1, "tau_1"], matrix_params[, "tau_1"])),
    "tau_2" = as.numeric(c(matrix_params[1, "tau_2"], matrix_params[, "tau_2"])),
    "p_0" = as.numeric(c(matrix_params[1, "p_0"], matrix_params[, "p_0"]))
  )
  k_plot <- ggplot(params_data, aes(x = day)) +
    geom_line(aes(y = k_1, color = "k_1")) +
    geom_line(aes(y = k_2, color = "k_2"))

  tau_plot <- ggplot(params_data, aes(x = day)) +
    geom_line(aes(y = tau_1, color = "tau_1")) +
    geom_line(aes(y = tau_2, color = "tau_2"))

  #outputs
  out_list <- list()
  out_list$performance <- perf_out
  out_list$cost_vec <- cost_vec
  out_list$perf_plot <- plot_perf(obs_perf, perf_out)
  out_list$params_plots <- k_plot + tau_plot
  return(out_list)
} 
```

```{r sanity check, warning=FALSE, eval = FALSE}
set.seed(1)
days_test <- 200
perf_sim_1 <- c(500, perf_tv(p_0 = 500,
          k_1 = 1,
          tau_1 = 25, 
          k_2 = 2, 
          tau_2 = 10,
          days = days_test,
          training_stim = list("constant", 100))$performance
) 
noise_fn <- function(x, sd){
  x+rnorm(1, mean = 0, sd = sd)
}

perf_sim_1 = purrr::map_dbl(perf_sim_1, noise_fn, sd = 10)

# this is one off
samp_index <- c(sample(c(1:100), 50), sample(c(101:days_test), 30))
actual_perf <- rep(NA, days_test)
for (i in samp_index) {
  actual_perf[[i]] = perf_sim_1[[i+1]]
}

model_perf <- new_pred_perf(
  c(500, 1,2, 25, 10),
  c(rep(100, days_test)),
  actual_perf,
  list("type" = "RMPE", "p" = 40, "alpha" = .95, "window" = Inf),
  list("type" = "lim_cost", "lambda" = 1)
)
model_perf$perf_plot 
model_perf$params_plots
model_perf$cost_vec
```

```{r testing}
set.seed(4)
days_test <- 200
perf_sim_1 <- c(500, perf_tv(p_0 = 500,
          k_1 = 1,
          tau_1 = 25, 
          k_2 = 2, 
          tau_2 = 10,
          days = days_test,
          training_stim = list("constant", 100))$performance
) 
noise_fn <- function(x,  
                     sd){
  x+rnorm(1, mean = 0, sd = sd)
}

perf_sim_1 = purrr::map_dbl(perf_sim_1, noise_fn, sd = 20)

# this is one off
samp_index <- c(sample(c(1:100), 50), sample(c(101:days_test), 10))
actual_perf <- rep(NA, days_test)
for (i in samp_index) {
  actual_perf[[i]] = perf_sim_1[[i+1]]
  
}

params_matrix_1 <- as.matrix(expand.grid(p_0 = 500,
                                  k_1 = c(1,2,3,4),
                                  k_2 = c(2,4,6,8),
                                  tau_1 = c(5:35),
                                  tau_2 = c(5:35))
)

res_1 <- min_params_new(params_matrix_1,
               training_load = c(rep(100,days_test)),
               actual_perf,
               c(500,1,2,25,10),
               lambda = .4)
res_1

```
## Algorithm Pseudo-code, not complete
<!-- ```{=latex} -->
<!-- \RestyleAlgo{ruled} -->
<!-- \begin{algorithm} -->
<!-- \caption{The Update Algorithm} -->
<!-- \KwData{  -->
<!-- Training Load, a vector \texttt{w} of length $n$ \\ -->
<!-- Observed Performance, \texttt{obs perf} a vector of length $n$ \\ -->
<!-- Initial Parameters, a vector \texttt{init params} $ = (p_0, k_1, k_2, \tau_1,\tau_2)$ \\ -->
<!-- Settings for the Grid of values to search over, $S$ \\ -->
<!-- Coeficent term for the Limit Cost term, a $\lambda>0$  -->
<!-- } -->
<!-- \KwResult{A list containing: \\ -->
<!-- a vector of the predicted performance \\ -->
<!-- a vector of the cost of the parameters at each of the observed performances \\ -->
<!-- a plot of the predicted performance and the observed performance \\ -->
<!-- plots of the parameters across time -->
<!-- } -->
<!-- \\ -->
<!-- Initializations\; -->
<!-- $\verb|curr_params| \gets \verb|init_params|$ -->


<!-- \end{algorithm} -->
<!-- ``` -->

Let $z^{(t)}=(p_0^t, k_1^t, k_2^t, \tau_1^t, \tau_2^t)$ be the set of parameters on day $t$. For the sake of pseudo-code, set a cost function
$$
C(\text{Error}(1,\dots, i), w^{(t)}, w^{(t-1)}) = f(\text{Error}(1,\dots,i)) + g(w^{(t)}, w^{(t-1)})
$$
where $\text{Error}(1,\dots,i)$ is the $i$ dimensional vector that has the error for the $1$ through $i$th performance days, $f$ is some function that penalizes error, and the actual performance, like a scaler times SSE, MSE, or RMSE, and $g$ is a function that penalizes the distance between the two parameter sets in some way. We wish to estimate the time varying parameters through Algorithm 1 .

```{=latex}
\RestyleAlgo{ruled}
\begin{algorithm}
\caption{Parameter estimation algorithm}
\KwData{ 
Training Load, a vector \texttt{w} of length $n$ \\
Observed Performance, \texttt{obs perf} a vector of length $n$ \\
Initial Parameters, a vector $z^{(0)}$ \\
Settings for the Grid of values to search over, $S$ \\
Coeficent term for the Limit Cost term, a $\lambda>0$ 
}
\KwResult{A vector of the predicted performance}
\For{$i=1,\hdots, n$}{
  \eIf{\text{Observed Performance at} $i$ \text{is a number}}{
  $G\gets \text{a grid of sets of parameters determined by some conditions}$ \\
  $w^{(i)} = \text{argmin}_{w\in G} C(\text{Error}(1,\hdots, i-1), z, z^{(i-1)})$ \\
  compute $P(i)$ with $w^{(i)}$
  }{
  compute $P(i)$ with $w^{(i-1)}$
  }
}
\end{algorithm}
```
Include here how exactly these algorithms work, this is still a work in progress.

# Testing New Algorthm on simulated data
## basic testing
As a sanity check, we give the algorithm the true values of the parameters
as the initial parameters, tested aginst the the actual performance with no noise
```{r sanity check098, warning=FALSE, eval = FALSE}
set.seed(1)
days_test <- 200
perf_sim_1 <- c(500, perf_tv(p_0 = 500, k_1 = 1, tau_1 = 25, k_2 = 2, 
          tau_2 = 10, days = days_test, 
          training_stim = list("constant", 100))$performance
) 
noise_fn <- function(x,  
                     sd){
  x+rnorm(1, mean = 0, sd = sd)
}

# perf_sim_1 = purrr::map_dbl(perf_sim_1, noise_fn, sd = 20)

# this is one off
samp_index <- c(sample(c(1:100), 30), sample(c(101:days_test), 10))
actual_perf <- rep(NA, days_test)
for (i in samp_index) {
  actual_perf[[i]] = perf_sim_1[[i+1]]
  
}

model_perf <- new_pred_perf_1(
  c(500, 1,2, 25, 10),
  c(rep(100, days_test)),
  actual_perf,
  lambda = 10
)
model_perf$perf_plot 
model_perf$params_plots

```
This checks out. It has passed the sanity check. The algorithm gives the same 
resutls no matter what the value of `lambda` is.
 

## adding noise
Running the same simulation with gaussian noise to the simulated performance, 
with standard deviation of $20$. We will see that increasing lambda smoothes 
out the performance curves. This first simulation is with `lambda = 0` which 
just finds the minimum parameters with respect to SSE. 
```{r noise simulation 1, eval = FALSE}
set.seed(5)
perf_sim_1 = purrr::map_dbl(perf_sim_1, noise_fn, sd = 10)
samp_index <- c(sample(c(1:100), 50), sample(c(101:days_test), 10))
actual_perf <- rep(NA, days_test)
for (i in samp_index) {
  actual_perf[[i]] = perf_sim_1[[i+1]]
}
model_perf <- new_pred_perf(
  c(500, 1,2, 25, 10),
  c(rep(100, days_test)),
  actual_perf,
  lambda = .3
)
model_perf$perf_plot 
model_perf$params_plots
```
This kind of wanders off

```{r noise simulation 2, eval = FALSE}
model_perf <- new_pred_perf(
  c(500, 1,2, 25, 10),
  c(rep(100, days_test)),
  actual_perf,
  lambda = .1
)
model_perf$perf_plot 
model_perf$params_plots
```
The wandering is fixed when we introduce some penalty for the distance function
```{r noise simulation 3, eval = FALSE}
model_perf <- new_pred_perf(
  c(500, 1,2, 25, 10),
  c(rep(100, days_test)),
  actual_perf,
  lambda = .5
)
model_perf$perf_plot 
model_perf$params_plots
```
## More complicated simulation 
Working on simulating a situation where the parameter change over time, but 
the simulated perforamance level is not realistic. Right now, the algorthm 
cannot handle this case. 
```{r time-varying simulation 1, eval = FALSE}
set.seed(2)
days_test <- 500
perf_sim_2 <- c(500, perf_tv(p_0 = 500,
          k_1 = c(1,1,1.1),
          tau_1 = c(15, 20, 10), 
          k_2 = c(2, 2, 2), 
          tau_2 = c(5,7, 20),
          change_days = c(100, 200), 
          days = days_test,
          training_stim = list("constant", 100))$performance
)
samp_index <- c(sample(c(1:100), 10), sample(c(101:days_test), 20))
actual_perf <- rep(NA, days_test)
for (i in samp_index) {
  actual_perf[[i]] = perf_sim_2[[i]]
}

plot_check <- plot_perf(actual_perf,
                        perf_sim_2) +
  labs(title = "Check that sampling from the simulation works")
plot_check

model_perf_2 <- new_pred_perf(
  c(500, 1,2, 25, 10),
  c(rep(100, days_test)),
  actual_perf,
  lambda = 10
)
model_perf_2$perf_plot
model_perf_2$params_plots

```
It does a pretty bad job. This is because the algorithm, at every new data point, 
tries to fit the best Time-*Invariant* curve. In this situation, all of the 
Time-Invariant functions don't fit well when the parameters first change. 

# Applying to Real Data
This is a work in progress. Right now, the algorithm cannot handle this case
This runs the 
```{r real data 1, eval = FALSE}
real_data <- data_2
day_vec_real <- real_data[[1]]
training_load_real <- real_data[[2]]
obs_perf_real <- real_data[[3]]
init_params_real <- c(262, .1, .1, 11, 11)
obs_new_alg <- new_pred_perf(
  init_params_real,
  training_load_real,
  obs_perf_real,
  lambda = 1
)

obs_new_alg$perf_plot
obs_new_alg$params_plots
```
Here, it does not change because the algorithm has decided that all of the 
options that we have told it to search over are all bad so it stuck with
the initial estimate which is probably a good sign.

