---
title: "Exploring sensitivity of Time-varing model"
output: html_notebook
---
testing RLS algorithm 

Packages and such
```{r}
library(ggplot2)
library(devtools)
library(tibble)
load_all()

set.seed(100)
```

Create two different sets of parameters and training loads, and a tibble that
will be used later. 
```{r}
params_1 = c(250,.1,.1,25,15)
params_2 = c(250,.5,.5,25,1)
training_load <- c(rep(100,100))
shortened_training_load <- training_load[1:50]
first_length <- length(shortened_training_load)
second_legth <- length(training_load)-length(shortened_training_load)
#to graph actual parameter values against the predicted ones
actual_param_tib <- tibble(
  "day" = 1:length(training_load),
  "p_0" = c(rep(params_1[[1]], first_length),
            rep(params_2[[1]], second_legth)
            ),
  "k_1" = c(rep(params_1[[2]], first_length),
            rep(params_2[[2]], second_legth)
            ),
  
  "k_2" = c(rep(params_1[[3]], first_length),
            rep(params_2[[3]], second_legth)
            ),
  "tau_1" = c(rep(params_1[[4]], first_length),
            rep(params_2[[4]], second_legth)
            ),
  "tau_2" = c(rep(params_1[[5]], first_length),
            rep(params_2[[5]], second_legth)
            )
)

```


Creating a model (with the time invariant model) where the first 50 days use the 
first set of paramters and the last 50 days use the second set. 


```{r}
non_noisy_performance_1 <- predictedPerformance(params_1, shortened_training_load)
non_noisy_performance_2 <- predictedPerformance(params_2, training_load)
non_noisy_perf = c(non_noisy_performance_1, non_noisy_performance_2[51:100])

```


Trying to recover the parameters with the time-varying model. Thought it was easier to make a function 
`prediction_with_alpha(performance, alpha, ...)` that plots the RLS algorithm --  with `alpha=alpha` and `predicted_performance = predicted parameter` -- given in `...`against that acutal paramters. 
```{r, eval=FALSE}

lists_to_tibble <- function(list_of_lists){
  len=length(list_of_lists)
  list_0 <- c(rep(0,len))
  out=tibble(
    "day" = 1:len,
    "p_0" = list_0,
    "k_1" = list_0,
    "k_2" = list_0,
    "tau_1" = list_0,
    "tau_2" = list_0
  )
  for(i in 1:len){
    for(j in 2:6){
     out[i,j] = list_of_lists[[i]][[j-1]] 
    }
  }
  out
}

prediction_with_alpha <- function(performance, alpha, ...){
  prediction_non_noisy <- RLS_predicted_performance(
      training_load,
      performance,
      250,
      alpha = alpha,
      delta = 1000,
      c(1,50),
      1,
      c(1,50),
      1
    )
  parameter_tibble_pred <- lists_to_tibble(prediction_non_noisy[[2]])
  plot <- ggplot(parameter_tibble_pred, aes(x = day, y = ...))+
    geom_line()+
    geom_line(data = actual_param_tib, color= "red")
  plot
}

prediction_with_alpha(non_noisy_perf, .0001, tau_1)
prediction_with_alpha(non_noisy_perf, .0001, tau_2)

# prediction_non_noisy <- RLS_predicted_performance(
#       training_load,
#       non_noisy_perf,
#       250,
#       alpha = .0001,
#       delta =1000,
#       c(1,50),
#       1,
#       c(1,50),
#       1
#     )
# prediction_non_noisy[[2]]
```
After some time, `tau_1` and `tau_2` flip between the acutal values. They also
take a unique value, so they are "trading" in some sense. 


Sometimes, both `k_1` and `k_2` are negative while `tau_1` and `tau_2` seem to filp
Prehaps the matrix of SSE is symmetric. Can then reduce the computational load. This is true:

```{r, eval=FALSE}
prediction_non_noisy <- RLS_predicted_performance(
      training_load,
      non_noisy_perf,
      250,
      alpha = .0001,
      delta = 1000,
      c(1,50),
      1,
      c(1,50),
      1
    
#This is the matrix for the RLS model SSE on day 5
(symm_matrix <- as.data.frame(prediction_non_noisy[[4]][5,,]))
```



Changing values of alpha effects how the algorithm recovers the parameter.
In the given value, .00000000000000001, the algorithm can recover the first
set of parameters in 3 data points, For reasonable values (like one or one or
two decimal points) the data can recover the first set in 7 data points. However,
for incredibly small values, the model converges on the wrong values, but more
values can still recover the origional parameters, just at the slow 7 data point rate

prehaps to predict paramters, we should throw out old data (which does have a recursive algorithm in the
same book that I have).



Applying noise to this model (with helper function that makes noise)
```{r}
noise <- function(x,  sd){
  x+rnorm(1, mean = 0, sd = sd)
}

noisy_perf = purrr::map_dbl(non_noisy_perf, noise, sd = 1)
```


The noise function and a check that it works:
```{r}
performance_tib = tibble::tibble(
  day = c(1:100),
  non_noisy = non_noisy_perf,
  noisy = noisy_perf 
)
 
# plot <- ggplot(performance_tib, aes(x = day, y = non_noisy))+
#   geom_line(color = "red")+
#   geom_line(aes(y=noisy), color = "blue")
# plot
```



Running RLS algorithm with the noisy performance
```{r}
prediction_noisy <- RLS_predicted_performance(
      training_load,
      noisy_perf,
      250,
      alpha = .0000001,
      delta = 1000,
      c(1,50),
      1,
      c(1,50),
      1
    )
#prediction_noisy[[2]]
view(prediction_noisy[[4]][5,,])
```
This is giving weird results; the parameters are not being recovered. 





